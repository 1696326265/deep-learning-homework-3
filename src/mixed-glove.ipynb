{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class DiRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, dropout_rate):\n",
    "        super(DiRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        \n",
    "        # bidirectional设为True即得到双向循环神经网络\n",
    "        self.encoder = nn.LSTM(input_size = embed_size, \n",
    "                    hidden_size = num_hiddens, \n",
    "                    num_layers = num_layers,\n",
    "                    dropout = dropout_rate,\n",
    "                    bidirectional = False)\n",
    "        self.decoder = nn.Linear(num_hiddens, vocab_size) # 初始时间步和最终时间步的隐藏状态作为全连接层输入\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # inputs的形状是(批量大小, 词数)，因为LSTM需要将序列长度(seq_len)作为第一维，所以将输入转置后\n",
    "        # 再提取词特征，输出形状为(词数, 批量大小, 词向量维度)\n",
    "        embeddings = self.embedding(inputs.permute(1, 0))\n",
    "        # rnn.LSTM只传入输入embeddings，因此只返回最后一层的隐藏层在各时间步的隐藏状态。\n",
    "        # outputs形状是(词数, 批量大小, 2 * 隐藏单元个数)\n",
    "        outputs, _ = self.encoder(embeddings) # output, (h, c)\n",
    "        # 连结初始时间步和最终时间步的隐藏状态作为全连接层输入。它的形状为\n",
    "        # (批量大小, 2 * 隐藏单元个数)。\n",
    "#         print(outputs.shape)\n",
    "        output = outputs.permute(1, 0, 2)\n",
    "#         print(output.shape)\n",
    "        outs = output.reshape(output.size(0)*output.size(1), output.size(2))\n",
    "#         print(outs.shape)\n",
    "        ret = self.decoder(outs)\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.datasets import WikiText2\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter\n",
    "from torchtext.vocab import Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus(object):\n",
    "    def __init__(self, train_batch_size=20, eval_batch_size=10, bptt=35):\n",
    "        self.bptt = bptt\n",
    "        train_iter = WikiText2(split='train')\n",
    "        self.tokenizer = get_tokenizer('basic_english')\n",
    "        counter = Counter()\n",
    "        for line in train_iter:\n",
    "            counter.update(self.tokenizer(line))\n",
    "        self.vocab = Vocab(counter)\n",
    "        train_iter, val_iter, test_iter = WikiText2()\n",
    "        train_data = self.data_process(train_iter)\n",
    "        val_data = self.data_process(val_iter)\n",
    "        test_data = self.data_process(test_iter)\n",
    "\n",
    "        self.train_data = self.batchify(train_data, train_batch_size)\n",
    "        self.val_data = self.batchify(val_data, eval_batch_size)\n",
    "        self.test_data = self.batchify(test_data, eval_batch_size)\n",
    "\n",
    "    def data_process(self, raw_text_iter):\n",
    "        data = [torch.tensor([self.vocab[token] for token in self.tokenizer(item)],\n",
    "                           dtype=torch.long) for item in raw_text_iter]\n",
    "        return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "\n",
    "    def batchify(self, data, batch_size):\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        # Divide the dataset into batch_size parts.\n",
    "        nbatch = data.size(0) // batch_size\n",
    "        # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "        data = data.narrow(0, 0, nbatch * batch_size)\n",
    "        # Evenly divide the data across the batch_size batches.\n",
    "        data = data.view(batch_size, -1).t().contiguous()\n",
    "        return data.to(device)\n",
    "\n",
    "    def get_batch(self, source, i):\n",
    "        seq_len = min(self.bptt, len(source) - 1 - i)\n",
    "        data = source[i:i+seq_len]\n",
    "        target = source[i+1:i+1+seq_len].reshape(-1)\n",
    "        return data, target\n",
    "\n",
    "    def get_ntokens(self):\n",
    "        return len(self.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import data\n",
    "import model\n",
    "class args():\n",
    "    emsize = 100\n",
    "    nhid = 200\n",
    "    nlayers = 5\n",
    "    lr = 2\n",
    "    epochs = 1000\n",
    "    batch_size = 32\n",
    "    bptt = 256\n",
    "    dropout = 0.5\n",
    "    tied = False\n",
    "    seed = 1234\n",
    "    cuda = True\n",
    "    log_interval = 100\n",
    "    save = 'model.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = Corpus(train_batch_size=args.batch_size,\n",
    "                     eval_batch_size=args.batch_size,\n",
    "                     bptt=args.bptt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data_loader.train_data\n",
    "val_data = data_loader.val_data\n",
    "test_data = data_loader.test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(source, i, evaluation=False):\n",
    "    seq_len = min(args.bptt, len(source) - 1 - i)\n",
    "#     data = Variable(source[i:i+seq_len], volatile=evaluation)\n",
    "#     target = Variable(source[i+1:i+1+seq_len].view(-1))\n",
    "    data = source[i:i+seq_len].t()\n",
    "    tmp = source[i+1:i+1+seq_len].t()\n",
    "#     print(data.shape,tmp.shape)\n",
    "    target = tmp.reshape(-1)\n",
    "    return data, target\n",
    "#     return data.clone().detach(), target.clone().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64062\n",
      "torch.Size([32, 256]) torch.Size([8192]) tensor([ 3878,     9,  1953,    20,    81,   314,  1054,     6,     2,   207,\n",
      "            4,    20,  1878,     3,     2,  1503,   314,    32,  1062,    44,\n",
      "         3783,     3, 16499,     2,   154,   123,   150,  1985,     6,  3074,\n",
      "          599, 16329,   259,  7098,  6385,     0,     4,    43,    45,  4006,\n",
      "            9,  2594,     6, 10097,   598,     3,   249,     2,     0,  3184,\n",
      "            3,    33,    11,     9,   724, 11309,   832,     4,    39,    14,\n",
      "          269,   205,     3,  1878,     3,  3638,    41,    40,  6334,    62,\n",
      "           82,   188,    23,     3,    23,   101,  6286,    41,  1084,     9,\n",
      "          185,  1295,     7, 23326,  1970,     4,     2,   913,    26,     2,\n",
      "         1503,   314,    11,  7228,     3,   588,     8, 22864,  1185,     7,\n",
      "           31,   687,     8,     0,     2,   317,   441,    12,    16,  2693,\n",
      "         2451,     7,  3638,     4,    14,   276,   343,     3,  1878,     3,\n",
      "            2,  1503,  2417,   173,     5,  3991,     7,  3652,   573,     2,\n",
      "         3890,     5,     2,   661,     5, 23579,     3,     7,     2,  1151,\n",
      "            5, 23326,     4,     0,     8,    15,     2,    12, 23579,  8001,\n",
      "           12,     3,     2,  1079,   573,    77,  1610,    22,   236,     8,\n",
      "            2,   428,   708,    21,     6,  1285,    22,   236,     8,     9,\n",
      "          211,    73,  3892,    20,     2,  1503,  1530,     0,   239,     7,\n",
      "        23325,    26,  1079,    21,  7129,  3019,     5, 23054,  3638, 10512,\n",
      "            4,     2,   590,     3,   728,     0,     3,    11,  1206,    27,\n",
      "            2, 21886,     5, 22612,   123,     0,  3519,     6,  6295,    20,\n",
      "         2505,     0,     0,     3,    53,    11,    73,  9453,    67,    31,\n",
      "        14464,    61,     2,   481,    11,    98,  1133,    83,     4,     2,\n",
      "         1079,    30,   843,    20,    36,  2505,    17,     2,  1786,    30,\n",
      "            8,  2099,    95,    60,     3,   409,  2296,    17,    69,  4641,\n",
      "            3,   506,     6,   360,   242,     3], device='cuda:0') tensor(242, device='cuda:0')\n",
      "False\n",
      "torch.Size([32, 256]) torch.Size([8192]) tensor([   30,   145,     4,    60,     5,     2,  3107,    30,  4641,     4,\n",
      "            2,  8001,    11, 11953,     7,   226,  2305,    20,     2,   154,\n",
      "         1273,  2403,     3,   468,  7152,  9206,     3,     6,    49,    56,\n",
      "         1226,  8008,  5282,     4,    73,  2720,    13,  1792,    50,    52,\n",
      "           87,     7,     9,  1047,     5,  3385,     5, 18678, 11652, 17785,\n",
      "            0,     0,    17,  5017,    17,     2, 19328,    30,  1133,    83,\n",
      "           20,  3638, 10950,     3,    40,  1503,   336,     4,     7,   574,\n",
      "            3,   488,   769,    17,     2,  5017, 19467,    20,  1503,   314,\n",
      "          336,    30, 18920,     8, 12848,     2,  2640,     5,     2,  1503,\n",
      "          119,  4626,     4,  1503,  3356,     0,     0,  8968,     2, 23579,\n",
      "         8001,     7,  3484,    20, 13429,   100,  3332,     6,   365,   377,\n",
      "            5,     2,  1503,   319,  3991,  1064,    17,  1133,    83,     2,\n",
      "         8001,     4,     0,    12,    16,  1047,    11,   327,     7,     2,\n",
      "         1503,  1758,  1530,     0,     6,    73,     7,     9,   245,  1793,\n",
      "          116,     5,     2,  3356,    12,    16,  5218,     4,    20,  2305,\n",
      "            3,  3638,    30,    45,  6186,  2529,  2330,     6,  3286,     7,\n",
      "           31,   687,     8, 20284,     2,  2529,  5580,     7,     2,  1503,\n",
      "          336,     4,     0,     0, 22546,     0,    22,     0,  3286,   270,\n",
      "           18,  3533,    21,    11,     9,  1838,   420,  2674,     7,     2,\n",
      "         2437,   478,     3,    15,     2,  1503,  2019,     8, 13684,     6,\n",
      "            0,     2,  4869,   441,     3,     7,   361,     8, 21929,     2,\n",
      "         3638,    26,    44,  2529,   750,     4,  9943,     3,  9660,    12,\n",
      "           16,  2060,     5,  6855,   597,  2529,  1503,  2250,    11,  1821,\n",
      "            7,  2305,    20,     2,    55,   745,     3, 22167,     4, 10122,\n",
      "            3,     0,     3,  9861,     3,     6,     9,  1518,     5,     0,\n",
      "           41,    60,    30,  4440,   310,  7347], device='cuda:0') tensor(310, device='cuda:0')\n",
      "False\n",
      "torch.Size([32, 256]) torch.Size([8192]) tensor([    7,  3652,   426,  1690,  1718,   174,     4,   430,     4,     0,\n",
      "            7,  2977,     4,    36,   854,     7,  6910,   301,     8,  2959,\n",
      "           20,  1503,  2250,   121,     2,  8280,   207,     3,     9,     0,\n",
      "         1986,     5,     2,  1690,    12,    16, 28465,     4,  1342,    19,\n",
      "            2,  1003,     5,     2, 23579,  8001,     6,    17,     5,  4890,\n",
      "         3638,     0,   109,  2305,     6,   115,  2154,     3,     2, 16389,\n",
      "         1299,     7,  3652,    73,  2073,     8,     2, 18794,     5,     2,\n",
      "         1503,   207,     7,  2154,     4,     9,  1503,  3356,  1748,    10,\n",
      "           10,    10,   505, 10543,     6, 13118,    22,  2154,    41,  1245,\n",
      "           21,    10,    10,    10,   170,     7,  8280,     3,     2,    12,\n",
      "         1387,  3712,   590,    12,  1526,     5,     2,  1503,  2510,   565,\n",
      "            3,    33,    11,   523,     7,     2,   228,  1686,     3,     6,\n",
      "            2,  3712,  2781,    22,     0,    21,     3,     9,   255,    13,\n",
      "         1234,  3954,     3,   716,     8, 13692,     2,  2563,  2746,     4,\n",
      "           43,    32,  1133,    83,  1843,     0,     6, 14073,   121,   314,\n",
      "         4810,     3,    90,    15,     2,   427,    14,     2,     0,   414,\n",
      "          750,    17,  1115,   116,  6467,    14,   244,   188,     3,  2416,\n",
      "            3,     6,     2,   427,    14,     2, 11223,  1035,    27,     0,\n",
      "            7,   214,     5,     2,   157,    86,     4,     2,   427,    14,\n",
      "            2,  1503,   372, 22403,  3641,     2,   300,     5,     2,  2563,\n",
      "         2746,     7,    36,  5797,     4, 22403,    22,   249,    46,     9,\n",
      "         9668,  1970,    21,    11,  4603,     8,  1107,  8280,    19,   669,\n",
      "            8,    34,  3347,     7,  5722,     4,    20,     2,    59,     5,\n",
      "            2, 21003,  1871,     3,   329,    23,     3,    23,   101,  1797,\n",
      "            0,    32,    52,   311,     4,  1059,  2563,  2746,     7,  1503,\n",
      "         2530,    32,  4914,  3584,    13,   128], device='cuda:0') tensor(13, device='cuda:0')\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__\n",
    "print(train_data.size(0))\n",
    "for batch, i in enumerate(range(0, train_data.size(0) - 1, args.bptt)):\n",
    "    data, targets = get_batch(train_data, i)\n",
    "    print(data.shape,targets.shape,data[-1],targets[-3])\n",
    "    print(data.requires_grad)\n",
    "    if (batch>=2): break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28783"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_loader.vocab.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = len(data_loader.vocab.itos)\n",
    "# model = RNNModel(args.model, ntokens, args.emsize, args.nhid,\n",
    "#         args.nlayers, args.dropout, args.tied)\n",
    "model = DiRNN(ntokens, args.emsize, args.nhid, args.nlayers, args.dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torchtext.vocab as Vocab\n",
    "glove_vocab = Vocab.GloVe(name='6B', dim=100, cache=os.path.join(\"data\",\"glove\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 265062)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(glove_vocab.itos), glove_vocab.stoi['999th']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 <unk>\n",
      "1 <pad>\n",
      "2 the\n",
      "3 ,\n",
      "4 .\n",
      "5 of\n",
      "6 and\n",
      "7 in\n",
      "8 to\n",
      "9 a\n",
      "10 =\n",
      "11 was\n"
     ]
    }
   ],
   "source": [
    "for i, word in enumerate(data_loader.vocab.itos):\n",
    "    print(i, word)\n",
    "    if i>10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1028 oov words.\n"
     ]
    }
   ],
   "source": [
    "def load_pretrained_embedding(l, pretrained_vocab):\n",
    "    \"\"\"从预训练好的vocab中提取出words对应的词向量\"\"\"\n",
    "    embed = torch.zeros(l, pretrained_vocab[0].shape[0]) # 初始化为0\n",
    "    oov_count = 0 # out of vocabulary\n",
    "    for i, word in enumerate(data_loader.vocab.itos):\n",
    "        try:\n",
    "            idx = pretrained_vocab.stoi[word]\n",
    "            embed[i, :] = pretrained_vocab.vectors[idx]\n",
    "        except KeyError:\n",
    "            oov_count += 1\n",
    "    if oov_count > 0:\n",
    "        print(\"There are %d oov words.\" % oov_count)\n",
    "#     for i in range(len(vec)):\n",
    "#         embed[i] = vec[i]\n",
    "    return embed\n",
    "\n",
    "model.embedding.weight.data.copy_(load_pretrained_embedding(ntokens, glove_vocab))\n",
    "model.embedding.weight.requires_grad = True # 直接加载预训练好的, 所以不需要更新它"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.cuda:\n",
    "    model = model.to(torch.device('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DiRNN(\n",
       "  (embedding): Embedding(28783, 100)\n",
       "  (encoder): LSTM(100, 200, num_layers=5, dropout=0.5)\n",
       "  (decoder): Linear(in_features=200, out_features=28783, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=args.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_source):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_words = 0\n",
    "    for i in range(0, data_source.size(0) - 1, args.bptt):\n",
    "        data, targets = get_batch(data_source, i)\n",
    "        \n",
    "        output = model(data)\n",
    "        loss = criterion(output, targets)\n",
    "\n",
    "        total_loss += loss.data * targets.shape[0]\n",
    "        total_words += targets.shape[0]\n",
    "    total_loss = float(total_loss.to(torch.device('cpu')))\n",
    "#     print(total_loss,total_words)\n",
    "    return total_loss / total_words, math.exp(total_loss / total_words)\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    total_words = 0\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, args.bptt)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        model.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.data * targets.shape[0]\n",
    "        total_words += targets.shape[0]\n",
    "        \n",
    "#         if (batch % args.log_interval == 0 and batch > 0):\n",
    "#             cur_loss = total_loss / args.log_interval\n",
    "#             elapsed = time.time() - start_time\n",
    "#             print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "#                     'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "#                 233, batch, len(train_data) // args.bptt, args.lr,\n",
    "#                 elapsed * 1000 / args.log_interval, cur_loss, math.exp(cur_loss)))\n",
    "#             total_loss = 0\n",
    "#             start_time = time.time()\n",
    "\n",
    "    cur_loss = total_loss / total_words\n",
    "    print('train {:10d} words | loss {:5.5f} | ppl {:5.5}'.format(total_words, cur_loss, math.exp(cur_loss)))\n",
    "    return cur_loss, math.exp(cur_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "Losstrain = []\n",
    "Lossval = []\n",
    "Losstest = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round :  0    2021-05-19 09:50:35\n",
      "train    2049952 words | loss 6.38019 | ppl 590.04\n",
      "  valid 454.11489 | test 425.52069\n",
      "--------------------------------------------------------------------------------\n",
      "Round :  1    2021-05-19 09:52:42\n",
      "train    2049952 words | loss 6.33356 | ppl 563.16\n",
      "  valid 508.25429 | test 472.89789\n",
      "--------------------------------------------------------------------------------\n",
      "Round :  2    2021-05-19 09:54:48\n",
      "train    2049952 words | loss 6.31754 | ppl 554.21\n",
      "  valid 438.05496 | test 410.52348\n",
      "--------------------------------------------------------------------------------\n",
      "Round :  3    2021-05-19 09:56:55\n",
      "train    2049952 words | loss 6.30438 | ppl 546.96\n",
      "  valid 475.06683 | test 442.55797\n",
      "--------------------------------------------------------------------------------\n",
      "Round :  4    2021-05-19 09:59:01\n",
      "train    2049952 words | loss 6.29037 | ppl 539.35\n",
      "  valid 417.35248 | test 389.60191\n",
      "--------------------------------------------------------------------------------\n",
      "Round :  5    2021-05-19 10:01:09\n",
      "train    2049952 words | loss 6.26825 | ppl 527.55\n",
      "  valid 429.11029 | test 399.57133\n",
      "--------------------------------------------------------------------------------\n",
      "Round :  6    2021-05-19 10:03:15\n",
      "train    2049952 words | loss 6.25881 | ppl 522.6\n",
      "  valid 426.60892 | test 397.07568\n",
      "--------------------------------------------------------------------------------\n",
      "Round :  7    2021-05-19 10:05:21\n",
      "train    2049952 words | loss 6.24787 | ppl 516.91\n",
      "  valid 467.30605 | test 435.77400\n",
      "--------------------------------------------------------------------------------\n",
      "Round :  8    2021-05-19 10:07:28\n",
      "train    2049952 words | loss 6.23857 | ppl 512.12\n",
      "  valid 439.18338 | test 407.97271\n",
      "--------------------------------------------------------------------------------\n",
      "Round :  9    2021-05-19 10:09:35\n",
      "train    2049952 words | loss 6.21991 | ppl 502.66\n",
      "  valid 471.12582 | test 438.19451\n",
      "--------------------------------------------------------------------------------\n",
      "Round :  10    2021-05-19 10:11:42\n",
      "train    2049952 words | loss 6.21798 | ppl 501.69\n",
      "  valid 503.12268 | test 466.36040\n",
      "--------------------------------------------------------------------------------\n",
      "Round :  11    2021-05-19 10:13:49\n",
      "train    2049952 words | loss 6.20688 | ppl 496.15\n",
      "  valid 450.95681 | test 418.48836\n",
      "--------------------------------------------------------------------------------\n",
      "Round :  12    2021-05-19 10:15:56\n",
      "train    2049952 words | loss 6.19871 | ppl 492.12\n",
      "  valid 427.87000 | test 398.92703\n",
      "--------------------------------------------------------------------------------\n",
      "Round :  13    2021-05-19 10:18:02\n",
      "train    2049952 words | loss 6.18726 | ppl 486.51\n",
      "  valid 412.93154 | test 382.95092\n",
      "--------------------------------------------------------------------------------\n",
      "Round :  14    2021-05-19 10:20:10\n",
      "train    2049952 words | loss 6.18073 | ppl 483.35\n",
      "  valid 451.19774 | test 418.30907\n",
      "--------------------------------------------------------------------------------\n",
      "Round :  15    2021-05-19 10:22:16\n",
      "train    2049952 words | loss 6.17190 | ppl 479.09\n",
      "  valid 410.50222 | test 380.36144\n",
      "--------------------------------------------------------------------------------\n",
      "Round :  16    2021-05-19 10:24:23\n",
      "train    2049952 words | loss 6.16153 | ppl 474.15\n",
      "  valid 440.91773 | test 406.69781\n",
      "--------------------------------------------------------------------------------\n",
      "Round :  17    2021-05-19 10:26:29\n",
      "train    2049952 words | loss 6.14769 | ppl 467.64\n",
      "  valid 415.51138 | test 383.19765\n",
      "--------------------------------------------------------------------------------\n",
      "Round :  18    2021-05-19 10:28:36\n",
      "train    2049952 words | loss 6.14482 | ppl 466.29\n",
      "  valid 368.37778 | test 342.98748\n",
      "--------------------------------------------------------------------------------\n",
      "Round :  19    2021-05-19 10:30:43\n",
      "train    2049952 words | loss 6.13404 | ppl 461.3\n",
      "  valid 399.36674 | test 369.78746\n",
      "--------------------------------------------------------------------------------\n",
      "Round :  20    2021-05-19 10:32:50\n",
      "train    2049952 words | loss 6.12888 | ppl 458.92\n",
      "  valid 347.31388 | test 322.94274\n",
      "--------------------------------------------------------------------------------\n",
      "Round :  21    2021-05-19 10:34:57\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-6b31893aeb43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#     print('  valid ',evaluate(val_data))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#     print('  test  ',evaluate(test_data))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtrainl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mvall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtestl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-d9be84516c07>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib64/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib64/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "for T in range(args.epochs):\n",
    "    print('Round : ',T,\"  \",time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())))\n",
    "#     train()\n",
    "#     print('  valid ',evaluate(val_data))\n",
    "#     print('  test  ',evaluate(test_data))\n",
    "    trainl, trainp = train()\n",
    "    vall, valp = evaluate(val_data)\n",
    "    testl, testp = evaluate(test_data)\n",
    "    \n",
    "    Losstrain.append(trainp)\n",
    "    Lossval.append(valp)\n",
    "    Losstest.append(testp)\n",
    "    np.save('model-glove-train',np.array(Losstrain))\n",
    "    np.save('model-glove-val',np.array(Lossval))\n",
    "    np.save('model-glove-test',np.array(Losstest))\n",
    "    \n",
    "    print('  valid {:5.5f} | test {:5.5f}'.format(valp, testp))\n",
    "    print('-'*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
